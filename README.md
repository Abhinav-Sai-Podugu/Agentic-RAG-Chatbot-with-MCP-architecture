# ü§ñ Agentic RAG Chatbot

An intelligent document Q&A system built with Agent-Based Architecture and Model Context Protocol (MCP) for seamless multi-format document processing and retrieval-augmented generation.


## üåü Features

### üìö Multi-Format Document Support

- ‚úÖ PDF - Extracts text from all pages

- ‚úÖ DOCX - Processes Word documents

- ‚úÖ PPTX - Handles PowerPoint presentations

- ‚úÖ CSV - Intelligent row grouping and header processing

- ‚úÖ TXT/Markdown - Smart paragraph and section-based chunking


## üèóÔ∏è Agentic Architecture

Three specialized agents communicate via Model Context Protocol (MCP):

- IngestionAgent - Parses and preprocesses documents

- RetrievalAgent - Handles embeddings and semantic search

- LLMResponseAgent - Generates contextual responses


## üîÑ Model Context Protocol (MCP)

Structured message passing between agents

Complete traceability with unique trace IDs

Error handling and status tracking

Real-time message flow visualization


## üéØ Advanced Features

Vector Store Integration - Semantic similarity search

Multi-turn Conversations - Maintains chat history

Source Attribution - Shows which document chunks were used

Real-time Processing - Live agent communication tracking

Responsive UI - Modern chat interface with Streamlit


## üèóÔ∏è Architecture

```bash
graph TD
    A[User Upload Documents] --> B[UI Layer - Streamlit]
    B --> C[Coordinator]
    C --> D[IngestionAgent]
    C --> E[RetrievalAgent] 
    C --> F[LLMResponseAgent]
    
    D -->|MCP Message| E
    E -->|MCP Message| F
    F -->|MCP Message| C
    
    E --> G[Vector Store]
    F --> H[OpenRouter API]
    
    subgraph "MCP Messages"
        I[DOC_PARSED]
        J[STORAGE_COMPLETE]
        K[RETRIEVAL_RESULT]
        L[LLM_RESPONSE]
    end
```

## üì° MCP Message Flow

```bash
{
  "sender": "RetrievalAgent",
  "receiver": "LLMResponseAgent",
  "type": "RETRIEVAL_RESULT",
  "trace_id": "abc-123-def",
  "timestamp": "2024-01-20T10:30:00Z",
  "payload": {
    "retrieved_context": ["relevant chunk 1", "relevant chunk 2"],
    "context_metadata": [{"source_file": "doc.pdf", "chunk_id": "doc_chunk_1"}],
    "query": "What are the key metrics?"
  }
}
```

## üìÅ Project Structure

```bash
agentic_rag_chatbot/
‚îÇ
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ coordinator.py          # MCP message orchestration
‚îÇ   ‚îú‚îÄ‚îÄ ingestion_agent.py      # Document parsing agent
‚îÇ   ‚îú‚îÄ‚îÄ retrieval_agent.py      # Vector storage & retrieval
‚îÇ   ‚îî‚îÄ‚îÄ llm_response_agent.py   # Response generation
‚îÇ
‚îú‚îÄ‚îÄ mcp/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ message.py              # MCP message utilities
‚îÇ
‚îú‚îÄ‚îÄ vectorstore/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ store.py                # Vector database implementation
‚îÇ
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ app.py                  # Streamlit frontend
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ parser_utils.py         # Document parsers
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ settings.py             # Configuration settings
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ run.py                      # Application entry point
```

## üöÄ Quick Setup

### 1. Clone Repository
   
```bash
git clone https://github.com/yourusername/agentic-rag-chatbot.git
cd agentic-rag-chatbot
```

### 2. Install Dependencies
   
```bash
pip install -r requirements.txt
```

### 3. Environment Setup

   Create a .env file:
   
```bash
# Copy example environment file
cp .env.example .env

# Edit with your API key
OPENROUTER_API_KEY=your_openrouter_api_key_here
```

### 4. Run Application

```bash
# Using run.py
python run.py

# Or directly with Streamlit
streamlit run ui/app.py
```

### 5. Access Application
  
   Open your browser and navigate to: http://localhost:8501

üìã Requirements

```bash
streamlit>=1.28.0
sentence-transformers>=2.2.2
scikit-learn>=1.3.0
PyPDF2>=3.0.1
python-docx>=0.8.11
python-pptx>=0.6.21
requests>=2.31.0
numpy>=1.24.0
python-dotenv>=1.0.0
```

## üîß Configuration

### Environment Variables

Create a .env file with:

```bash
# Required: OpenRouter API Key for LLM access
OPENROUTER_API_KEY=your_api_key_here

# Optional: Model Configuration
LLM_MODEL=mistralai/mistral-7b-instruct
EMBEDDING_MODEL=all-MiniLM-L6-v2
MAX_CHUNKS_RETRIEVAL=5
CHUNK_SIZE=500
CHUNK_OVERLAP=50
```

### Supported LLM Models

The system uses OpenRouter API and supports:

- mistralai/mistral-7b-instruct (default)

- anthropic/claude-instant-v1

- And many more available through OpenRouter (Preffered: Use lighter models)


## üíª Usage

### Basic Workflow

1. Upload Documents

- Use the sidebar to upload multiple files

- Supported formats: PDF, DOCX, PPTX, CSV, TXT, MD


2. Ask Questions

- Type your question in the chat input

- The system will process through all agents

- Get responses with source attribution


3. View Agent Communication

- Toggle "Show MCP Message Flow" in sidebar

- See real-time agent interactions

- Expand message details for debugging

### Example Queries

```bash
"What are the key metrics mentioned in the quarterly report?"
"Summarize the main points from the presentation slides"
"What data trends can you identify from the CSV file?"
"Compare the findings across all uploaded documents"
```

## üõ†Ô∏è Advanced Features

### MCP Message Tracing

Every operation is tracked with unique trace IDs:
- Monitor agent communication flow
- Debug processing pipeline
- Ensure message delivery

### Intelligent Document Chunking

- PDF: Page-based extraction with smart chunking
- DOCX: Paragraph-aware processing
- PPTX: Slide-based segmentation
- CSV: Header-aware row grouping
- TXT/MD: Section and paragraph-based splitting
